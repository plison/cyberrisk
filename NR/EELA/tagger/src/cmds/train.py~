# -*- coding: utf-8 -*-

import os
from datetime import datetime, timedelta, date
from src import Tagger, Model
from src.metric import Metric
from src.utils import Corpus, Embedding, Vocab
from src.utils.data import TextDataset, batchify
import torch
from torch.optim import Adam
from torch.optim.lr_scheduler import ExponentialLR
import csv

class Train(object):

    def add_subparser(self, name, parser):
        subparser = parser.add_parser(
            name, help='Train a model.'
        )
        subparser.add_argument('--buckets', default=64, type=int,
                               help='max num of buckets to use')
        subparser.add_argument('--punct', action='store_false', default=True,
                               help='whether to include punctuation')
        subparser.add_argument('--ftrain', default='data/ptb/train.conllx',
                               help='path to train file')
        subparser.add_argument('--fdev', default='data/ptb/dev.conllx',
                               help='path to dev file')
        subparser.add_argument('--fembed', default=False,
                               help='path to pretrained embeddings')
        subparser.add_argument('--n_char_embed', default=100, type=int,
                               help='char embedding size')
        subparser.add_argument('--unk', default='unk',
                               help='unk token in pretrained embeddings')
        subparser.add_argument('--load_model', default=None)
        subparser.add_argument('--load_vocab', default=None)
        subparser.add_argument("--n_lstm_nodes", default=None, type=int)
        subparser.add_argument("--n_lstm_layers", default=None, type=int)
        subparser.add_argument("--patience", default=20, type=int)
        
        
        return subparser

    def __call__(self, config):
        #if config.lstm_nodes is not None:
        #    
        #if config.lstm_layers is not None:
        #
        config.patience = config.patience
        config.n_lstm_hidden = config.n_lstm_nodes
        config.n_lstm_layers = config.n_lstm_layers
        print("Preprocess the data")
        train = Corpus.load(config.ftrain)
        dev = Corpus.load(config.fdev)

        print("Generating vocab.")
        vocab= Vocab.from_corpus(corpus=train, min_freq=1)
        if config.fembed:
            print("Loading embeddings.")
            vocab.read_embeddings(Embedding.load(config.fembed, config.unk, config.n_embed))
        else:
            print("Randomly initiliasing embeddings.")
            vocab.randomly_initialise_embeddings(config.n_embed)
        torch.save(vocab, config.vocab)

        config.update({
            'n_words': vocab.n_words,
            'n_tags': vocab.n_tags,
            'n_chars': vocab.n_chars,
            'pad_index': vocab.pad_index,
            'unk_index': vocab.unk_index
        })
        print(vocab)
       
        print("Load the dataset")
        x = vocab.numericalize(train)
        x1 = 0#10200
        x2 = len(x[0])
        X = (x[0][x1:x2], x[1][x1:x2], x[2][x1:x2])
        trainset = TextDataset(vocab.numericalize(train))
        devset = TextDataset(vocab.numericalize(dev))

        # set the data loaders
        train_loader = batchify(dataset=trainset,
                                batch_size=config.batch_size,
                                shuffle=True)
        dev_loader = batchify(dataset=devset,
                              batch_size=config.batch_size)
        print(f"{'train:':6} {len(trainset):5} sentences in total, "
              f"{len(train_loader):3} batches provided")
        print(f"{'dev:':6} {len(devset):5} sentences in total, "
              f"{len(dev_loader):3} batches provided")

        print("Creating model")
    
        tagger = Tagger(config, vocab.embeddings)
       
        if torch.cuda.is_available():
            tagger = tagger.cuda()
        print(f"{tagger}\n")

        model = Model(vocab, tagger)

        total_time = timedelta()
        best_e, best_metric = 1, 99.99
        best_train_metric = 99.99
        model.optimizer = Adam(model.tagger.parameters(),
                               config.lr,
                               (config.beta_1, config.beta_2),
                               config.epsilon)
        model.scheduler = ExponentialLR(model.optimizer,
                                        config.decay ** (1 / config.steps))
        epochs_since_improvement = 0

        total_params = sum(p.numel() for p in model.tagger.parameters())
        print ("Total number of parameters:", total_params,"\n")
        for epoch in range(1, config.epochs + 1):
            start = datetime.now()
            # train one epoch and update the parameters
            model.train(train_loader)

            print(f"Epoch {epoch} / {config.epochs}:")
            loss, train_metric = model.evaluate(train_loader, config.punct)
            print(f"{'train:':6} Loss: {loss:.4f} \t mse: {train_metric:.2f}")
            loss, dev_metric = model.evaluate(dev_loader, config.punct)
            print(f"{'dev:':6} Loss: {loss:.4f} \t mse: {dev_metric:.2f}")
            
            t = datetime.now() - start
            # save the model if it is the best so far
            if dev_metric < best_metric:
                best_e, best_metric = epoch, dev_metric
                best_train_metric = train_metric
                model.tagger.save(config.model)
                print(f"{t}s elapsed (saved)\n")
                epochs_since_improvement = 0
            else:
                epochs_since_improvement += 1
                print(f"{t}s elapsed\n")
            if epochs_since_improvement >= config.patience:
                print("Early stopping after stalled improvement.\n")
                break
            total_time += t
        

        
        print(f"max score of dev is {best_metric:.2f} at epoch {best_e}")
        print(f"average time of each epoch is {total_time / epoch}s")
        print(f"{total_time}s elapsed")
                                      
